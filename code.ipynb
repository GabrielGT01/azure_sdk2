{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# create a folder for the script files\n",
        "script_folder = 'src'\n",
        "os.makedirs(script_folder, exist_ok=True)\n",
        "print(script_folder, 'folder created')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "src folder created\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721000885479
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/insurance-training.py\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # function that reads the data\n",
        "    df = get_data(args.input_data)\n",
        "\n",
        "    # Clean the data for duplicates and outliers\n",
        "    new_data = clean_data(df)\n",
        "\n",
        "    # Split data for training and testing\n",
        "    X_train, X_test, y_train, y_test = split_data(new_data)\n",
        "\n",
        "    # Input data into model\n",
        "    data_model(X_train, X_test, y_train, y_test, args)\n",
        "\n",
        "\n",
        "# Function that reads the data\n",
        "def get_data(path):\n",
        "    print(\"Reading data ...\")\n",
        "    df = pd.read_csv(path)\n",
        "    return df\n",
        "\n",
        "\n",
        "def clean_data(data):\n",
        "    df_copy = data.copy()\n",
        "\n",
        "    # Check for duplicates\n",
        "    duplicates = df_copy.duplicated()\n",
        "    print(f'Number of duplicate rows: {duplicates.sum()}')\n",
        "\n",
        "    # Remove duplicate rows\n",
        "    df_copy = df_copy.drop_duplicates()\n",
        "\n",
        "    # Retrieve index location for prices greater than 50k and drop\n",
        "    high_price = df_copy[df_copy['charges'] >= 50000].index\n",
        "    df_copy.drop(high_price, axis=0, inplace=True)\n",
        "\n",
        "    # Label encoding the categorical columns\n",
        "    le_sex = LabelEncoder()\n",
        "    le_smoker = LabelEncoder()\n",
        "\n",
        "    # Fit and transform the 'sex' and 'smoker' columns\n",
        "    df_copy['sex'] = le_sex.fit_transform(df_copy['sex'])\n",
        "    df_copy['smoker'] = le_smoker.fit_transform(df_copy['smoker'])\n",
        "\n",
        "    # Apply pd.get_dummies to encode the 'region' column\n",
        "    df_encoded = pd.get_dummies(df_copy, columns=['region'], dtype=float)\n",
        "\n",
        "    return df_encoded\n",
        "\n",
        "\n",
        "def split_data(cleaned_data):\n",
        "    y = cleaned_data['charges'].values\n",
        "    cleaned_data.drop('charges', axis=1, inplace=True)\n",
        "    X = cleaned_data.values\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=0)\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def data_model(X_train, X_test, y_train, y_test, args):\n",
        "    # Start a new MLflow run\n",
        "    with mlflow.start_run():\n",
        "        # Model parameters\n",
        "        n_estimators = args.n_estimators\n",
        "        learning_rate = args.learning_rate\n",
        "        max_depth = args.max_depth\n",
        "        random_state = args.random_state\n",
        "\n",
        "        # Initialize and train the GradientBoostingRegressor\n",
        "        gbr = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth, random_state=random_state)\n",
        "        model = gbr.fit(X_train, y_train)\n",
        "\n",
        "        # Save the model\n",
        "        # Log the model with MLflow\n",
        "        mlflow.sklearn.log_model(model, \"model\")\n",
        "\n",
        "        # Predict on the test set\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Evaluate the model\n",
        "        mean_abs_error = mean_absolute_error(y_test, y_pred)\n",
        "        mean_sq_error = mean_squared_error(y_test, y_pred)\n",
        "        root_mean_sq_error = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        # Plot predicted vs actual values\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(y_test, y_pred, color='blue', edgecolor='k', alpha=0.7, s=100)\n",
        "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "        plt.xlabel('Actual Values')\n",
        "        plt.ylabel('Predicted Values')\n",
        "        plt.title('Predicted vs Actual Values')\n",
        "        plt.savefig('Regression-line.png')\n",
        "\n",
        "        # Log parameters and metrics\n",
        "        params = {\n",
        "            \"n_estimators\": n_estimators,\n",
        "            \"learning_rate\": learning_rate,\n",
        "            \"max_depth\": max_depth,\n",
        "            \"random_state\": random_state\n",
        "        }\n",
        "        mlflow.log_params(params)\n",
        "\n",
        "        metrics = {\n",
        "            \"mean_absolute_error\": mean_abs_error,\n",
        "            \"mean_squared_error\": mean_sq_error,\n",
        "            \"root_mean_squared_error\": root_mean_sq_error,\n",
        "            \"R2\": r2\n",
        "        }\n",
        "        mlflow.log_metrics(metrics)\n",
        "        mlflow.log_artifact(\"Regression-line.png\")\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    # Setup arg parser\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Add arguments\n",
        "    parser.add_argument(\"--input_data\", dest='input_data', type=str, required=True)\n",
        "    parser.add_argument(\"--n_estimators\", dest='n_estimators', type=int, default=100)\n",
        "    parser.add_argument(\"--learning_rate\", dest='learning_rate', type=float, default=0.1)\n",
        "    parser.add_argument(\"--max_depth\", dest='max_depth', type=int, default=3)\n",
        "    parser.add_argument(\"--random_state\", dest='random_state', type=int, default=123)\n",
        "\n",
        "    # Parse args\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Return args\n",
        "    return args\n",
        "\n",
        "\n",
        "# Run script\n",
        "if __name__ == \"__main__\":\n",
        "    # Add space in logs\n",
        "    print(\"\\n\\n\")\n",
        "    print(\"*\" * 60)\n",
        "\n",
        "    # Parse args\n",
        "    args = parse_args()\n",
        "\n",
        "    # Run main function\n",
        "    main(args)\n",
        "\n",
        "    # Add space in logs\n",
        "    print(\"*\" * 60)\n",
        "    print(\"\\n\\n\")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting src/insurance-training.py\n"
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721000888110
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a handle to workspace\n",
        "ml_client = MLClient.from_config(credential=credential)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721000888418
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import Input, Output\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml import command\n",
        "\n",
        "# Configure input and output\n",
        "my_job_inputs = {\n",
        "    \"local_data\": Input(type=AssetTypes.URI_FILE, path=\"azureml:insurance-price:1\")\n",
        "}\n",
        "\n",
        "\n",
        "# Configure job\n",
        "job = command(\n",
        "    code=\"./src\",\n",
        "    command=\"python insurance-training.py --input_data ${{inputs.local_data}}\",\n",
        "    inputs=my_job_inputs,\n",
        "    environment=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\",\n",
        "    compute=\"captgt0071\",\n",
        "    display_name=\"insurance_price\",\n",
        "    experiment_name=\"insurance_price_charge\"\n",
        ")\n",
        "\n",
        "# Submit job\n",
        "price_job = ml_client.create_or_update(job)\n",
        "view_job = price_job.studio_url\n",
        "print(\"Monitor your job at\", view_job)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n\u001b[32mUploading src (0.01 MBs): 100%|██████████| 5359/5359 [00:00<00:00, 66872.37it/s]\n\u001b[39m\n\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Monitor your job at https://ml.azure.com/runs/busy_glass_7sfcpx5b0m?wsid=/subscriptions/cda9116f-5326-4a9b-9407-bc3a4391c27c/resourcegroups/data_udemy/workspaces/dala_project&tid=aef6e45c-850f-4f38-a10b-1df3ad33cdb0\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1721000893210
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}